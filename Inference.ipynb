{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision.models._utils import IntermediateLayerGetter\n",
    "\n",
    "from models.modeling.deeplab import *\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "\n",
    "from models.model import JointModel\n",
    "from dataloader.vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    lr = 3e-4\n",
    "    batch_size = 64\n",
    "    num_workers = 4\n",
    "    image_encoder = \"deeplabv3_plus\"\n",
    "    num_layers = 1\n",
    "    num_encoder_layers = 1\n",
    "    dropout = 0.25\n",
    "    skip_conn = False\n",
    "    model_path = \"./saved_model/talk2car/jrm_baseline_drop_0.25_bs_64_el_1_sl_40_bce_0.58126.pth\"\n",
    "    dataroot = \"/ssd_scratch/cvit/kanishk/\"\n",
    "    glove_path = \"/ssd_scratch/cvit/kanishk/glove/\"\n",
    "    vocabulary_path=\"/home/kanishk/vigil/autonomous_grounding/dataloader/vocabulary.txt\"\n",
    "    dataset = \"talk2car\"\n",
    "    task = \"talk2car\"\n",
    "    split = \"val\"\n",
    "    max_len = 40 \n",
    "    image_dim = 448\n",
    "    mask_dim = 448\n",
    "    mask_thresh = 0.3\n",
    "    area_thresh = 0.4\n",
    "    topk = 10\n",
    "    metric = \"pointing_game\"\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda being used with 2 GPUs!!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(f'{device} being used with {n_gpu} GPUs!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating Vocabulary\n",
    "vocabulary = Vocabulary(args.vocabulary_path, args.glove_path, args.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "to_tensor = transforms.ToTensor()\n",
    "resize = transforms.Resize((args.image_dim, args.image_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([resize, to_tensor, normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_layers = {\"layer2\": \"layer2\", \"layer3\": \"layer3\", \"layer4\": \"layer4\"}\n",
    "\n",
    "model = DeepLab(num_classes=21, backbone=\"resnet\", output_stride=16)\n",
    "model.load_state_dict(torch.load(\"./models/deeplab-resnet.pth.tar\")[\"state_dict\"])\n",
    "\n",
    "image_encoder = IntermediateLayerGetter(model.backbone, return_layers)\n",
    "\n",
    "for param in image_encoder.parameters():\n",
    "    param.requires_grad_(False)\n",
    "    \n",
    "in_channels = 2048\n",
    "out_channels = 512\n",
    "stride = 2\n",
    "\n",
    "joint_model = JointModel(\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    stride=stride,\n",
    "    num_layers=args.num_layers,\n",
    "    num_encoder_layers=args.num_encoder_layers,\n",
    "    dropout=args.dropout,\n",
    "    skip_conn=args.skip_conn,\n",
    "    mask_dim=args.mask_dim,\n",
    ")\n",
    "\n",
    "if n_gpu > 1:\n",
    "    image_encoder = nn.DataParallel(image_encoder)\n",
    "    joint_model = nn.DataParallel(joint_model)  ## toggle this comment if you get key error\n",
    "\n",
    "state_dict = torch.load(args.model_path)\n",
    "if \"state_dict\" in state_dict:\n",
    "    state_dict = state_dict[\"state_dict\"]\n",
    "joint_model.load_state_dict(state_dict) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_model.to(device)\n",
    "image_encoder.to(device)\n",
    "\n",
    "image_encoder.eval();\n",
    "joint_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = input(\"Image path: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = input(\"Input Command: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Image.open(img_path)\n",
    "resized_image = np.array(input_image.resize((448, 448)))\n",
    "\n",
    "image = transform(input_image)\n",
    "\n",
    "phrase, phrase_mask = vocabulary.tokenize(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prepare for Model\n",
    "image = image.cuda(non_blocking=True).unsqueeze(0)\n",
    "image_mask = torch.ones(1, 14 * 14, dtype=torch.int64).cuda(non_blocking=True)\n",
    "\n",
    "phrase = phrase.unsqueeze(0).cuda(non_blocking=True)\n",
    "phrase_mask = phrase_mask.unsqueeze(0).cuda(non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input to Model\n",
    "\n",
    "with torch.no_grad():\n",
    "    image = image_encoder(image) \n",
    "    \n",
    "output_mask = joint_model(image, phrase, image_mask, phrase_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mask = output_mask.detach().cpu().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = float(input('threshold for mask: '))\n",
    "assert 0<= threshold < 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_seg = np.array(resized_image)[:] / 2\n",
    "\n",
    "predicts = (output_mask > threshold).numpy()\n",
    "\n",
    "im_seg[:, :, 0] += predicts.astype('uint8') * 100\n",
    "im_seg = im_seg.astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the Output\n",
    "\n",
    "figure = plt.figure(figsize=(20, 20))\n",
    "plt.imshow(im_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
