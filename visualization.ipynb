{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import skimage\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision.models._utils import IntermediateLayerGetter\n",
    "\n",
    "from models.modeling.deeplab import *\n",
    "from dataloader.talk2car import *\n",
    "\n",
    "import denseCRF\n",
    "import pydensecrf.densecrf as dcrf\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "\n",
    "from losses import Loss\n",
    "from models.model import JointModel\n",
    "\n",
    "from utils.im_processing import *\n",
    "from utils.metrics import *\n",
    "\n",
    "from summarizer import Summarizer\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    lr = 3e-4\n",
    "    batch_size = 64\n",
    "    num_workers = 4\n",
    "    image_encoder = \"deeplabv3_plus\"\n",
    "    num_layers = 1\n",
    "    num_encoder_layers = 1\n",
    "    dropout = 0.25\n",
    "    skip_conn = False\n",
    "    model_path = \"./saved_model/talk2car/deeplabv3_plus_talk2car_1_bce_0.52071.pth\"\n",
    "    loss = \"bce\"\n",
    "    dataroot = \"/ssd_scratch/cvit/kanishk/\"\n",
    "    glove_path = \"/ssd_scratch/cvit/kanishk/glove/\"\n",
    "    dataset = \"talk2car\"\n",
    "    task = \"talk2car\"\n",
    "    split = \"val\"\n",
    "    seq_len = 25\n",
    "    image_dim = 448\n",
    "    mask_dim = 448\n",
    "    mask_thresh = 0.3\n",
    "    area_thresh = 0.4\n",
    "    topk = 10\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda being used with 2 GPUs!!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(f'{device} being used with {n_gpu} GPUs!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing dataset\")\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "to_tensor = transforms.ToTensor()\n",
    "resize = transforms.Resize((args.image_dim, args.image_dim))\n",
    "\n",
    "tokenizer = None\n",
    "\n",
    "if args.dataset == \"referit\":\n",
    "    val_dataset = ReferDataset(\n",
    "        data_root=args.dataroot,\n",
    "        dataset=args.task,\n",
    "        transform=transforms.Compose([resize, to_tensor, normalize]),\n",
    "        annotation_transform=transforms.Compose([ResizeAnnotation(args.mask_dim)]),\n",
    "        split=args.split,\n",
    "        max_query_len=args.seq_len,\n",
    "        glove_path=args.glove_path,\n",
    "    )\n",
    "else:\n",
    "    val_dataset = Talk2Car(\n",
    "        root=args.dataroot,\n",
    "        split=args.split,\n",
    "        transform=transforms.Compose([resize, to_tensor, normalize]),\n",
    "        mask_transform=transforms.Compose([ResizeAnnotation(args.mask_dim)]),\n",
    "        glove_path=args.glove_path\n",
    "    )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, shuffle=True, batch_size=1, num_workers=0, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_layers = {\"layer2\": \"layer2\", \"layer3\": \"layer3\", \"layer4\": \"layer4\"}\n",
    "\n",
    "model = DeepLab(num_classes=21, backbone=\"resnet\", output_stride=16)\n",
    "model.load_state_dict(torch.load(\"./models/deeplab-resnet.pth.tar\")[\"state_dict\"])\n",
    "\n",
    "image_encoder = IntermediateLayerGetter(model.backbone, return_layers)\n",
    "\n",
    "for param in image_encoder.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = 2048\n",
    "out_channels = 512\n",
    "stride = 2\n",
    "\n",
    "joint_model = JointModel(\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    stride=stride,\n",
    "    num_layers=args.num_layers,\n",
    "    num_encoder_layers=args.num_encoder_layers,\n",
    "    dropout=args.dropout,\n",
    "    skip_conn=args.skip_conn,\n",
    "    mask_dim=args.mask_dim,\n",
    ")\n",
    "\n",
    "state_dict = torch.load(args.model_path)\n",
    "if \"state_dict\" in state_dict:\n",
    "    state_dict = state_dict[\"state_dict\"]\n",
    "joint_model.load_state_dict(state_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_gpu > 1:\n",
    "    image_encoder = nn.DataParallel(image_encoder)\n",
    "\n",
    "joint_model.to(device)\n",
    "image_encoder.to(device)\n",
    "\n",
    "image_encoder.eval();\n",
    "joint_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = Loss(args)\n",
    "val_iter = iter(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mask_IOU(masks, target, thresh=0.3):\n",
    "    assert(target.shape[-2:] == masks.shape[-2:])\n",
    "    temp = ((masks>thresh) * target)\n",
    "    intersection = temp.sum()\n",
    "    union = (((masks>thresh) + target) - temp).sum()\n",
    "    return intersection, union\n",
    "\n",
    "def meanIOU(m, gt, t):\n",
    "    temp = ((m > t)*gt)\n",
    "    inter = temp.sum()\n",
    "    union = ((m > t) + gt - temp).sum()\n",
    "    return inter/union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_iter = iter(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = val_dataset.__len__()\n",
    "indx = random.choice(range(data_len))\n",
    "batch = val_dataset.__getitem__(indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = random.sample(range(data_len), data_len)\n",
    "indices = list(range(data_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_counter = Counter()\n",
    "with PdfPages(f'results_full_{args.mask_thresh}.pdf') as pdf:\n",
    "    for indx in indices:\n",
    "        \n",
    "        batch = val_dataset.__getitem__(indx)\n",
    "\n",
    "        img = batch[\"image\"].cuda(non_blocking=True).unsqueeze(0)\n",
    "\n",
    "        # phrase = batch[\"phrase\"].cuda(non_blocking=True)\n",
    "        # phrase_mask = batch[\"phrase_mask\"].cuda(non_blocking=True)\n",
    "\n",
    "        ### Custom Phrase ###\n",
    "        # batch[\"orig_phrase\"] = \"park near the left car\"\n",
    "        # batch[\"orig_phrase\"] = summarizer(batch[\"orig_phrase\"])\n",
    "        phrase, phrase_mask = val_dataset.vocabulary.tokenize(batch[\"orig_phrase\"])\n",
    "        phrase = phrase.unsqueeze(0).cuda(non_blocking=True)\n",
    "        phrase_mask = phrase_mask.unsqueeze(0).cuda(non_blocking=True)\n",
    "\n",
    "        gt_mask = batch[\"seg_mask\"]\n",
    "        gt_mask = gt_mask.squeeze(dim=1)\n",
    "\n",
    "        batch_size = img.shape[0]\n",
    "        img_mask = torch.ones(batch_size, 14 * 14, dtype=torch.int64).cuda(non_blocking=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img = image_encoder(img)  # ['out']\n",
    "\n",
    "        output_mask = joint_model(img, phrase, img_mask, phrase_mask)\n",
    "\n",
    "        output_mask = output_mask.detach().cpu().squeeze()\n",
    "        mask_out = output_mask[0]\n",
    "\n",
    "        loss = loss_func(output_mask, gt_mask).item()\n",
    "        inter, union = compute_mask_IOU(output_mask, gt_mask)\n",
    "\n",
    "        score = inter / union\n",
    "\n",
    "        orig_image = batch[\"orig_image\"] #.numpy()\n",
    "        orig_phrase = batch[\"orig_phrase\"]\n",
    "\n",
    "        # orig_mask = batch[\"orig_mask\"]\n",
    "\n",
    "        example = {\n",
    "            \"image\": orig_image,\n",
    "            \"phrase\": orig_phrase,\n",
    "            \"mask_gt\": gt_mask,\n",
    "            \"mask_pred\": output_mask,\n",
    "            # \"orig_mask\": orig_mask,\n",
    "        }\n",
    "\n",
    "        image = example[\"image\"] #[0]\n",
    "        phrase = example[\"phrase\"]\n",
    "        mask_gt = example[\"mask_gt\"]#[0]\n",
    "        mask_pred = example[\"mask_pred\"]#[0]\n",
    "\n",
    "        # im = (image * 255).astype('uint8')\n",
    "        im = image\n",
    "\n",
    "        iou = []\n",
    "        thr = []\n",
    "        cum_sum = []\n",
    "\n",
    "        t_ = 0.0\n",
    "\n",
    "        best_t = t_\n",
    "        best_iou = 0\n",
    "\n",
    "        while t_ < 1:\n",
    "            miou = meanIOU(output_mask, gt_mask, t_)\n",
    "            cum_sum.append((output_mask > t_).sum())\n",
    "            iou.append(miou)\n",
    "            thr.append(t_)\n",
    "\n",
    "            if best_iou < miou:\n",
    "                best_iou = miou\n",
    "                best_t = t_\n",
    "\n",
    "            t_ += 0.05\n",
    "\n",
    "        if best_t == 0:\n",
    "            best_t += 0.0001\n",
    "\n",
    "        best_counter.update({best_t:1})\n",
    "\n",
    "        ## Prediction\n",
    "        im_seg = im[:] / 2\n",
    "        predicts = (mask_pred > args.mask_thresh).numpy()\n",
    "\n",
    "        ## Combined\n",
    "        im_comb = im[:] / 2\n",
    "\n",
    "        ## Ground Truth\n",
    "        im_gt = im[:] / 2\n",
    "        gt = (mask_gt > 0).numpy()\n",
    "\n",
    "        ## Prediction \n",
    "        im_seg[:, :, 0] += predicts.astype('uint8') * 100\n",
    "        im_seg = im_seg.astype('uint8')\n",
    "\n",
    "        ## Best Threshold\n",
    "        im_comb[:, :, 0] += predicts.astype('uint8') * 100\n",
    "        im_comb[:, :, 1] += gt.astype('uint8') * 100\n",
    "        im_comb = im_comb.astype('uint8')\n",
    "\n",
    "        ## Ground Truth\n",
    "        im_gt[:, :, 1] += gt.astype('uint8') * 100\n",
    "        im_gt = im_gt.astype('uint8')\n",
    "\n",
    "\n",
    "        figure, axes = plt.subplots(nrows=2, ncols=2, figsize=(30, 30))\n",
    "\n",
    "        axes[0, 0].imshow(im)\n",
    "        axes[0, 0].set_title(f\"Image:: {indx}\", fontsize=20)\n",
    "        axes[0, 0].axis(\"off\")\n",
    "\n",
    "        axes[0, 1].imshow(im_gt)\n",
    "        axes[0, 1].set_title(\"Ground Truth MAP\", fontsize=20)\n",
    "        axes[0, 1].axis(\"off\")\n",
    "\n",
    "        axes[1, 0].imshow(im_seg)\n",
    "        axes[1, 0].set_title(f\"Predicted MAP (Threshold: {args.mask_thresh})\", fontsize=20)\n",
    "        axes[1, 0].axis(\"off\")\n",
    "\n",
    "        axes[1, 1].imshow(im_comb)\n",
    "        axes[1, 1].set_title(f\"Combined (predicted + ground_truth)\", fontsize=20)\n",
    "        axes[1, 1].axis(\"off\")\n",
    "\n",
    "        figure.suptitle(f\"{phrase}\", y=0.51, fontsize=25)\n",
    "\n",
    "        pdf.savefig(figure)\n",
    "\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.05: 493, 0.0001: 313, 0.1: 68, 0.2: 45, 0.15000000000000002: 37, 0.25: 25, 0.35: 23, 0.3: 22, 0.39999999999999997: 21, 0.5499999999999999: 18, 0.44999999999999996: 16, 0.65: 14, 0.7000000000000001: 13, 0.7500000000000001: 12, 0.6: 11, 0.8500000000000002: 9, 0.49999999999999994: 8, 0.9000000000000002: 6, 0.9500000000000003: 5, 0.8000000000000002: 4})\n"
     ]
    }
   ],
   "source": [
    "print(best_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/kanishk/vigil/autonomous_grounding/dataloader/talk2car_w_rpn_no_duplicates.json\", \"rb\") as f:\n",
    "    data = json.load(f)[args.split]\n",
    "    data = {int(k): v for k, v in data.items()}\n",
    "img_dir = os.path.join(args.dataroot, \"imgs\")\n",
    "mask_dir = os.path.join(args.dataroot, \"mask_image_bin\")\n",
    "\n",
    "data_len = len(data)\n",
    "print(f'Total Examples in {args.split} set: {data_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = random.choice(range(data_len))\n",
    "command = data[index]['command']\n",
    "\n",
    "img_file = data[index]['img']\n",
    "img_path = os.path.join(img_dir, img_file)\n",
    "\n",
    "mask_file = f\"gt_img_ann_{args.split}_{index}.png\"\n",
    "mask_path = os.path.join(mask_dir, mask_file)\n",
    "\n",
    "img = Image.open(img_path)\n",
    "img = np.array(img)\n",
    "\n",
    "mask = Image.open(mask_path)\n",
    "mask = np.array(mask)\n",
    "\n",
    "img_overlay = img[:] / 2\n",
    "mask_ = (mask > 0)\n",
    "img_overlay[:, :, 0] += mask_.astype('uint8') * 100\n",
    "img_overlay = img_overlay.astype('uint8')\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(20, 20))\n",
    "\n",
    "ax[0].imshow(img)\n",
    "ax[0].set_title(command)\n",
    "ax[0].set_axis_off()\n",
    "\n",
    "ax[1].imshow(img_overlay)\n",
    "ax[1].set_title(command)\n",
    "ax[1].set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_command = summarizer(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Original Command:: {command}')\n",
    "print(f'Simple Command:: {simple_command}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"stop\", \"slow\", \"wait\", \"then\", \"speed\", \"change\", \"continue\", \"follow\", \"u-turn\", \"once\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(command):\n",
    "    count = 0\n",
    "    words = []\n",
    "    for key in keywords:\n",
    "        if key in command:\n",
    "            words.append(key)\n",
    "            count += 1\n",
    "    return count, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_temporal_count = 0\n",
    "temporal_count = 0\n",
    "\n",
    "max_word_count = 0\n",
    "\n",
    "# command_dict = {}\n",
    "# for key in keywords:\n",
    "#     command_dict[key] = 0\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "for ind in range(data_len):\n",
    "    command = data[ind]['command']\n",
    "    \n",
    "    for key in keywords:\n",
    "        if key in command:\n",
    "            counter.update({key:1})\n",
    "    \n",
    "    inter_count, inter_words = intersection(command)\n",
    "    \n",
    "    if inter_count == 0:\n",
    "        non_temporal_count += 1\n",
    "    else:\n",
    "        temporal_count += 1\n",
    "        if inter_count > max_word_count:\n",
    "            max_word_count = inter_count\n",
    "            print(command)\n",
    "            print(inter_words, inter_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(max_word_count, temporal_count, non_temporal_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counter, sum(counter.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
