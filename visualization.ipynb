{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import skimage\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchvision.models._utils import IntermediateLayerGetter\n",
    "\n",
    "from models.modeling.deeplab import *\n",
    "from dataloader.talk2car import *\n",
    "\n",
    "import denseCRF\n",
    "import pydensecrf.densecrf as dcrf\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "\n",
    "from losses import Loss\n",
    "from models.model import JointModel\n",
    "\n",
    "from utils.im_processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    lr = 3e-4\n",
    "    batch_size = 64\n",
    "    num_workers = 4\n",
    "    image_encoder = \"deeplabv3_plus\"\n",
    "    num_layers = 1\n",
    "    num_encoder_layers = 1\n",
    "    dropout = 0.25\n",
    "    skip_conn = False\n",
    "    model_path = \"./saved_model/talk2car/deeplabv3_plus_jrm_1_cmmlf_bce_l1_0.49439.pth\"\n",
    "    loss = \"bce\"\n",
    "    dataroot = \"/ssd_scratch/cvit/kanishk/\"\n",
    "    glove_path = \"/ssd_scratch/cvit/kanishk/glove/\"\n",
    "    dataset = \"talk2car\"\n",
    "    task = \"talk2car\"\n",
    "    split = \"val\"\n",
    "    seq_len = 25\n",
    "    image_dim = 448\n",
    "    mask_dim = 448\n",
    "    corrected_spell = False\n",
    "\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda being used with 2 GPUs!!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(f'{device} being used with {n_gpu} GPUs!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing dataset\")\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "to_tensor = transforms.ToTensor()\n",
    "resize = transforms.Resize((args.image_dim, args.image_dim))\n",
    "\n",
    "tokenizer = None\n",
    "\n",
    "if args.dataset == \"referit\":\n",
    "    val_dataset = ReferDataset(\n",
    "        data_root=args.dataroot,\n",
    "        dataset=args.task,\n",
    "        transform=transforms.Compose([resize, to_tensor, normalize]),\n",
    "        annotation_transform=transforms.Compose([ResizeAnnotation(args.mask_dim)]),\n",
    "        split=args.split,\n",
    "        max_query_len=args.seq_len,\n",
    "        glove_path=args.glove_path,\n",
    "    )\n",
    "else:\n",
    "    val_dataset = Talk2Car(\n",
    "        root=args.dataroot,\n",
    "        split=args.split,\n",
    "        transform=transforms.Compose([resize, to_tensor, normalize]),\n",
    "        mask_transform=transforms.Compose([ResizeAnnotation(args.mask_dim)]),\n",
    "        glove_path=args.glove_path\n",
    "    )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, shuffle=True, batch_size=1, num_workers=0, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_layers = {\"layer2\": \"layer2\", \"layer3\": \"layer3\", \"layer4\": \"layer4\"}\n",
    "\n",
    "model = DeepLab(num_classes=21, backbone=\"resnet\", output_stride=16)\n",
    "model.load_state_dict(torch.load(\"./models/deeplab-resnet.pth.tar\")[\"state_dict\"])\n",
    "\n",
    "image_encoder = IntermediateLayerGetter(model.backbone, return_layers)\n",
    "\n",
    "for param in image_encoder.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './saved_model/talk2car/deeplabv3_plus_jrm_1_cmmlf_bce_l1_0.49439.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-34e6cf202f91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"state_dict\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './saved_model/talk2car/deeplabv3_plus_jrm_1_cmmlf_bce_l1_0.49439.pth'"
     ]
    }
   ],
   "source": [
    "in_channels = 2048\n",
    "out_channels = 512\n",
    "stride = 2\n",
    "\n",
    "joint_model = JointModel(\n",
    "    in_channels=in_channels,\n",
    "    out_channels=out_channels,\n",
    "    stride=stride,\n",
    "    num_layers=args.num_layers,\n",
    "    num_encoder_layers=args.num_encoder_layers,\n",
    "    dropout=args.dropout,\n",
    "    skip_conn=args.skip_conn,\n",
    "    mask_dim=args.mask_dim,\n",
    ")\n",
    "\n",
    "state_dict = torch.load(args.model_path)\n",
    "if \"state_dict\" in state_dict:\n",
    "    state_dict = state_dict[\"state_dict\"]\n",
    "joint_model.load_state_dict(state_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_gpu > 1:\n",
    "    image_encoder = nn.DataParallel(image_encoder)\n",
    "\n",
    "joint_model.to(device)\n",
    "image_encoder.to(device)\n",
    "\n",
    "image_encoder.eval();\n",
    "joint_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = Loss(args)\n",
    "val_iter = iter(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mask_IOU(masks, target, thresh=0.3):\n",
    "    assert(target.shape[-2:] == masks.shape[-2:])\n",
    "    temp = ((masks>thresh) * target)\n",
    "    intersection = temp.sum()\n",
    "    union = (((masks>thresh) + target) - temp).sum()\n",
    "    return intersection, union\n",
    "\n",
    "def meanIOU(m, gt, t):\n",
    "    temp = ((m > t)*gt)\n",
    "    inter = temp.sum()\n",
    "    union = ((m > t) + gt - temp).sum()\n",
    "    return inter/union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_iter = iter(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = val_dataset.__len__()\n",
    "indx = random.choice(range(data_len))\n",
    "batch = val_dataset.__getitem__(indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = val_iter.next()\n",
    "\n",
    "img = batch[\"image\"].cuda(non_blocking=True).unsqueeze(0)\n",
    "\n",
    "# phrase = batch[\"phrase\"].cuda(non_blocking=True)\n",
    "# phrase_mask = batch[\"phrase_mask\"].cuda(non_blocking=True)\n",
    "\n",
    "### Custom Phrase ###\n",
    "# # batch[\"orig_phrase\"] = \"store on left next to hats blanket draped in front\"\n",
    "phrase, phrase_mask = val_dataset.vocabulary.tokenize(batch[\"orig_phrase\"])\n",
    "phrase = phrase.unsqueeze(0).cuda(non_blocking=True)\n",
    "phrase_mask = phrase_mask.unsqueeze(0).cuda(non_blocking=True)\n",
    "\n",
    "gt_mask = batch[\"seg_mask\"]\n",
    "gt_mask = gt_mask.squeeze(dim=1)\n",
    "\n",
    "batch_size = img.shape[0]\n",
    "img_mask = torch.ones(batch_size, 14 * 14, dtype=torch.int64).cuda(non_blocking=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    img = image_encoder(img)  # ['out']\n",
    "\n",
    "output_mask = joint_model(img, phrase, img_mask, phrase_mask)\n",
    "\n",
    "output_mask = output_mask.detach().cpu().squeeze()\n",
    "mask_out = output_mask[0]\n",
    "\n",
    "loss = loss_func(output_mask, gt_mask).item()\n",
    "inter, union = compute_mask_IOU(output_mask, gt_mask)\n",
    "    \n",
    "score = inter / union\n",
    "\n",
    "orig_image = batch[\"orig_image\"] #.numpy()\n",
    "orig_phrase = batch[\"orig_phrase\"]\n",
    "\n",
    "\n",
    "# orig_mask = batch[\"orig_mask\"]/\n",
    "\n",
    "example = {\n",
    "    \"image\": orig_image,\n",
    "    \"phrase\": orig_phrase,\n",
    "    \"mask_gt\": gt_mask,\n",
    "    \"mask_pred\": output_mask,\n",
    "    # \"orig_mask\": orig_mask,\n",
    "}\n",
    "\n",
    "image = example[\"image\"] #[0]\n",
    "phrase = example[\"phrase\"]\n",
    "mask_gt = example[\"mask_gt\"]#[0]\n",
    "mask_pred = example[\"mask_pred\"]#[0]\n",
    "\n",
    "# im = (image * 255).astype('uint8')\n",
    "im = image\n",
    "\n",
    "iou = []\n",
    "thr = []\n",
    "cum_sum = []\n",
    "\n",
    "t_ = 0.0\n",
    "\n",
    "best_t = t_\n",
    "best_iou = 0\n",
    "\n",
    "while t_ < 1:\n",
    "    miou = meanIOU(output_mask, gt_mask, t_)\n",
    "    cum_sum.append((output_mask > t_).sum())\n",
    "    iou.append(miou)\n",
    "    thr.append(t_)\n",
    "    \n",
    "    if best_iou < miou:\n",
    "        best_iou = miou\n",
    "        best_t = t_\n",
    "    \n",
    "    t_ += 0.05\n",
    "\n",
    "if best_t == 0:\n",
    "    best_t += 0.01\n",
    "\n",
    "im_seg = im[:] / 2\n",
    "predicts = (mask_pred > 0.4).numpy()\n",
    "print(predicts.shape)\n",
    "im_seg[:, :, 0] += predicts.astype('uint8') * 100\n",
    "im_seg = im_seg.astype('uint8')\n",
    "\n",
    "im_gt = im[:] / 2\n",
    "gt = (mask_gt > 0).numpy()\n",
    "im_gt[:, :, 0] += gt.astype('uint8') * 100\n",
    "im_gt = im_gt.astype('uint8')\n",
    "\n",
    "print(phrase)\n",
    "\n",
    "figure, axes = plt.subplots(nrows=1, ncols=3, figsize=(30, 30))\n",
    "\n",
    "axes[0].imshow(im_seg)\n",
    "axes[0].set_title(\"Prediction\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# image = batch[\"image\"][0].permute(1, 2, 0).numpy()\n",
    "\n",
    "axes[1].imshow(im)\n",
    "axes[1].set_title(phrase, loc='center', wrap=True)\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "axes[2].imshow(im_gt)\n",
    "axes[2].set_title(\"Ground Truth\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "print(f\"IOU overlab with ground truth: {score}, bestIOU: {best_iou}, best_t: {best_t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Command Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer\n",
    "\n",
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/kanishk/vigil/autonomous_grounding/dataloader/talk2car_w_rpn_no_duplicates.json\", \"rb\") as f:\n",
    "    data = json.load(f)[args.split]\n",
    "    data = {int(k): v for k, v in data.items()}\n",
    "img_dir = os.path.join(args.dataroot, \"imgs\")\n",
    "mask_dir = os.path.join(args.dataroot, \"mask_image_bin\")\n",
    "\n",
    "data_len = len(data)\n",
    "print(f'Total Examples in {args.split} set: {data_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = random.choice(range(data_len))\n",
    "command = data[index]['command']\n",
    "\n",
    "img_file = data[index]['img']\n",
    "img_path = os.path.join(img_dir, img_file)\n",
    "\n",
    "mask_file = f\"gt_img_ann_{args.split}_{index}.png\"\n",
    "mask_path = os.path.join(mask_dir, mask_file)\n",
    "\n",
    "img = Image.open(img_path)\n",
    "img = np.array(img)\n",
    "\n",
    "mask = Image.open(mask_path)\n",
    "mask = np.array(mask)\n",
    "\n",
    "img_overlay = img[:] / 2\n",
    "mask_ = (mask > 0)\n",
    "img_overlay[:, :, 0] += mask_.astype('uint8') * 100\n",
    "img_overlay = img_overlay.astype('uint8')\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1) #, figsize=(20, 20))\n",
    "\n",
    "ax[0].imshow(img)\n",
    "ax[0].set_title(command)\n",
    "ax[0].set_axis_off()\n",
    "\n",
    "ax[1].imshow(img_overlay)\n",
    "ax[1].set_title(command)\n",
    "ax[1].set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_command = model(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Original Command:: {command}')\n",
    "print(f'Simple Command:: {simple_command}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# command_list = []\n",
    "# for ind in range(data_len):\n",
    "#     orig_command = data[ind]['command']\n",
    "#     simple_command = model(orig_command)\n",
    "    \n",
    "#     if simple_command == '':\n",
    "#         simple_command = orig_command\n",
    "        \n",
    "#     if not (simple_command == orig_command):\n",
    "#         print(f'Old Command: {orig_command} |||||| New Command: {simple_command}')\n",
    "        \n",
    "#     command_list.append(simple_command)\n",
    "    \n",
    "#     if ind % 100 == 0:\n",
    "#         print(f'Processed {ind + 1} commands!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_a = 'get behind the white car in the left lane.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_b = model(orig_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_a == sent_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
